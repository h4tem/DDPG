{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48999453",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, you will study the effect of partial observability \n",
    "on the CartPoleContinuous-v1 environment, using either the DDPG or the TD3 algorithm.\n",
    "\n",
    "To emulate partial observability, you will design dedicated wrappers. Then you will study\n",
    "whether extending the input of the agent policy and critic with a memory of previous states\n",
    "and its output with action chunks can help solve the partial observability issue. This will\n",
    "also be achieved by designing other temporal extension wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39d97c",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbe005f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatem/deepdac/lib/python3.10/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=False)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e0af3",
   "metadata": {},
   "source": [
    "# Temporal modification wrappers\n",
    "\n",
    "The CartPoleContinuous-v1 environment is a custom extension of \n",
    "[the CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/) \n",
    "with continuous actions, between -1 and 1. It is defined in the bbrl_gymnasium library.\n",
    "\n",
    "As in CartPole, the state of the system contains 4 variables:\n",
    "- the position $x$,\n",
    "- the velocity $\\dot{x}$,\n",
    "- the angle of the pole $\\theta$,\n",
    "- the angular velocity $\\dot{\\theta}$.\n",
    "\n",
    "To emulate partial observability in CartPoleContinuous-v1, you will hide the $\\dot{x}$ and $\\dot{\\theta}$ features, \n",
    "by filtering them out of the state of the environment. This is implemented with the ```FeatureFilterWrapper```.\n",
    "\n",
    "To compensate for partial observability, you will extend the architecture of the agent \n",
    "with a memory of previous states and its output with action chunks.\n",
    "This is implemented with to wrappers, the ```ObsTimeExtensionWrapper``` and the ```ActionTimeExtensionWrapper```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b49f3e",
   "metadata": {},
   "source": [
    "## The FeatureFilterWrapper\n",
    "\n",
    "The FeatureFilterWrapper removes a feature from the output observation when calling the ```reset()``` and ```step(action)``` functions.\n",
    "The index of the removed feature is given when building the object.\n",
    "\n",
    "To hide the $\\dot{x}$ and $\\dot{\\theta}$ features from the CartPoleContinuous-v1 environment, \n",
    "the idea is to call the wrapper twice, using something like\n",
    "```env = FeatureFilterWrapper(FeatureFilterWrapper(inner_env, 3), 1)``` where ```inner_env``` is the CartPoleContinuous-v1 environment.\n",
    "\n",
    "### Exercise 1: code the FeatureFilterWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the observation space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b4ba0c7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "class FeatureFilterWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, index):\n",
    "        super().__init__(env)\n",
    "        self.index = index\n",
    "        self._adjust_observation_space()\n",
    "    \n",
    "    def _adjust_observation_space(self):\n",
    "        old_space = self.observation_space\n",
    "        low = np.delete(old_space.low, self.index)\n",
    "        high = np.delete(old_space.high, self.index)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=old_space.dtype)\n",
    "    \n",
    "    def observation(self,obs):\n",
    "        return np.delete(obs, self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c538da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02316633  0.00855443]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "env = FeatureFilterWrapper(FeatureFilterWrapper(env, 3),1)\n",
    "action = torch.randint(0,2,(1,)).item()\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93100d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.03707886, 0.04693683], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f63b4",
   "metadata": {},
   "source": [
    "## The ObsTimeExtensionWrapper\n",
    "\n",
    "When facing a partially observable environment, training with RL a reactive agent which just selects an action based on the current observation\n",
    "is not guaranteed to reach optimality. An option to mitigate this fundamental limitation is to equip the agent with a memory of the past.\n",
    "\n",
    "One way to do so is to use a recurrent neural network instead of a feedforward one to implement the agent: the neural network contains\n",
    "some memory capacity and the RL process may tune this internal memory so as to remember exactly what is necessary from the\n",
    "past observation. This has been done many times using an LSTM, see for instance \n",
    "[this early paper](https://proceedings.neurips.cc/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf).\n",
    "\n",
    "Another way to do so is to equip the agent with a list-like memory of the past observations \n",
    "and to extend the critic and policy to take as input the current observation and the previous ones.\n",
    "This removes the difficulty of learning an adequate representation of the past, but this results in \n",
    "enlarging the input size of the actor and critic networks. This can only be done if the required memory\n",
    "horizon to behave optimally is small enough.\n",
    "\n",
    "In the case of the CartPoleContinuous-v1 environment, one can see immediately that a memory of the previous\n",
    "observation is enough to compensate for the absence of the derivative features, since $\\dot{a} \\approx (a_{t} - a_{t-1})$.\n",
    "\n",
    "So we will extend the RL agent with a memory of size 1.\n",
    "\n",
    "Though it may not be intuitive at first glance, the simplest way to do so is to embed the environment into a wrapper\n",
    "which contains the required memory and produces the extended observations. This way, the RL agent will naturally be built\n",
    "with an extended observation space, and the wrapper will be in charge of concatenating the memorized\n",
    "observation from the previous step with the current observation received from the inner environment when calling the ```step(action)``` function.\n",
    "When calling the ```reset()``` function, the memory of observations should be reinitialized with null observations.\n",
    "\n",
    "### Exercise 2: code the ObsTimeExtensionWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the observation space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cefbada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "class ObsTimeExtensionWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self,env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self._adjust_observation_space()\n",
    "    \n",
    "    def _adjust_observation_space(self):\n",
    "        old_space = self.observation_space\n",
    "        low = np.tile(old_space.low,2)\n",
    "        high = np.tile(old_space.high,2)\n",
    "        self.observation_space = gym.spaces.Box(low=low,high=high, dtype=old_space.dtype)\n",
    "\n",
    "    def observation(self,obs):\n",
    "        if self.prev_obs is None:\n",
    "            obs = np.array([None,None,obs[0],obs[1]])\n",
    "            self.prev_obs = obs\n",
    "            return obs\n",
    "        else:\n",
    "            obs = np.concatenate([self.prev_obs[2:],obs])\n",
    "        self.prev_obs = obs\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12b793b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None None 0.04997177 -0.01313368] False\n",
      "[0.04997177 -0.01313368 0.053936079144477844 -0.019091971218585968] False\n",
      "[0.053936079144477844 -0.019091971218585968 0.06180652230978012\n",
      " -0.030986180528998375] False\n",
      "[0.06180652230978012 -0.030986180528998375 0.06577997654676437\n",
      " -0.03714822232723236] False\n",
      "[0.06577997654676437 -0.03714822232723236 0.0658600851893425\n",
      " -0.03765522316098213] False\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "env = ObsTimeExtensionWrapper(FeatureFilterWrapper(FeatureFilterWrapper(env, 3),1))\n",
    "for step in range(5):\n",
    "    action = torch.randint(0,2,(1,)).item()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(observation,terminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b16d16",
   "metadata": {},
   "source": [
    "## The ActionTimeExtensionWrapper\n",
    "\n",
    "It has been observed that, in partially observable environments, preparing to play a sequence of actions and only playing\n",
    "the first can be better than only preparing for one action. The difference comes from the fact that the critic evaluates\n",
    "sequences of actions, even if only the first is played in practice.\n",
    "\n",
    "Similarly to the ObsTimeExtensionWrapper, the corresponding behavior can be implemented with a wrapper.\n",
    "The size of the action space of the extended environment should be \n",
    "M times the size of the action space of the inner environment. This ensures that the policy and the critic\n",
    "will consider extended actions.\n",
    "Besides, the ```step(action)``` function should receive an extended actions of size M times the size of an action,\n",
    "and should only transmit the first action to the inner environment.\n",
    "\n",
    "\n",
    "### Exercise 3: code the ActionTimeExtensionWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the action space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4d0a90a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ActionTimeExtensionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, action_horizon=2):\n",
    "        super().__init__(env)\n",
    "        self.action_horizon = action_horizon\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.tile(env.action_space.low, self.action_horizon),\n",
    "            high=np.tile(env.action_space.high, self.action_horizon),\n",
    "            dtype=env.action_space.dtype\n",
    "        )\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17e8da",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e5a03",
   "metadata": {},
   "source": [
    "# Experimental study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b21da",
   "metadata": {},
   "source": [
    "To run the experiments below, you can use the [DDPG](https://arxiv.org/pdf/1509.02971.pdf) \n",
    "or [TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf) algorithms.\n",
    "\n",
    "You can just copy paste here the code you have used during the corresponding labs.\n",
    "We only provide a suggested set of hyper-parameters working well on the CartPoleContinuous-v1 environment for DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b08828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Get the current state $s_t$ and the chosen action $a_t$\n",
    "        obs = self.get((\"env/env_obs\", t))  # shape B x D_{obs}\n",
    "        action = self.get((\"action\", t))  # shape B x D_{action}\n",
    "\n",
    "        # Compute the Q-value(s_t, a_t)\n",
    "        obs_act = torch.cat((obs, action), dim=1)  # shape B x (D_{obs} + D_{action})\n",
    "        # Get the q-value (and remove the last dimension since it is a scalar)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf968e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1cb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b232c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77344ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feccdea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the (Torch) mse loss function\n",
    "# `mse(x, y)` computes $\\|x-y\\|^2$\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "def compute_critic_loss(cfg, reward: torch.Tensor, must_bootstrap: torch.Tensor, q_values: torch.Tensor, target_q_values: torch.Tensor):\n",
    "    \"\"\"Compute the DDPG critic loss from a sample of transitions\n",
    "\n",
    "    :param cfg: The configuration\n",
    "    :param reward: The reward (shape 2xB)\n",
    "    :param must_bootstrap: Must bootstrap flag (shape 2xB)\n",
    "    :param q_values: The computed Q-values (shape 2xB)\n",
    "    :param target_q_values: The Q-values computed by the target critic (shape 2xB)\n",
    "    :return: the loss (a scalar)\n",
    "    \"\"\"\n",
    "    # Compute temporal difference\n",
    "    # To be completed...\n",
    "    \n",
    "    q_val = q_values[0]\n",
    "    target_vals = target_q_values[1]\n",
    "\n",
    "    target = reward[0] + cfg[\"algorithm\"][\"discount_factor\"]*target_vals*must_bootstrap[0]\n",
    "    loss = mse(q_val,target)\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b78cff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(q_values):\n",
    "    \"\"\"Returns the actor loss\n",
    "\n",
    "    :param q_values: The q-values (shape 2xB)\n",
    "    :return: A scalar (the loss)\n",
    "    \"\"\"\n",
    "    # To be completed...\n",
    "\n",
    "    return -torch.sum(q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c0f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # we create the critic and the actor, but also an exploration agent to\n",
    "        # add noise and a target critic. The version below does not use a target\n",
    "        # actor as it proved hard to tune, but such a target actor is used in\n",
    "        # the original paper.\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        self.critic = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic/\")\n",
    "        self.target_critic = copy.deepcopy(self.critic).with_prefix(\"target-critic/\")\n",
    "\n",
    "        self.actor = ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # As an alternative, you can use `AddOUNoise`\n",
    "        noise_agent = AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # Define agents over time\n",
    "        self.t_actor = TemporalAgent(self.actor)\n",
    "        self.t_critic = TemporalAgent(self.critic)\n",
    "        self.t_target_critic = TemporalAgent(self.target_critic)\n",
    "\n",
    "        # Configure the optimizer\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer = setup_optimizer(cfg.critic_optimizer, self.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26769503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ddpg(ddpg: DDPG):\n",
    "    for rb in ddpg.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(ddpg.cfg.algorithm.batch_size)\n",
    "        ddpg.t_critic(rb_workspace, t=0, n_steps=2)\n",
    "        ddpg.t_actor(rb_workspace, t=0, n_steps=2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "                ddpg.t_target_critic(rb_workspace, t=0, n_steps=2)\n",
    "\n",
    "        q_values, terminated, reward, action, target_q_values = rb_workspace[\n",
    "                \"critic/q_value\", \"env/terminated\", \"env/reward\", \"action\", \"target-critic/q_value\"\n",
    "            ]\n",
    "        \n",
    "        # Determines whether values of the critic should be propagated\n",
    "        must_bootstrap = ~terminated\n",
    "\n",
    "        # Compute the critic loss\n",
    "       \n",
    "        # Critic update\n",
    "        # Compute critic loss\n",
    "        critic_loss = compute_critic_loss(ddpg.cfg, reward, must_bootstrap, q_values, target_q_values)\n",
    "       \n",
    "\n",
    "        # Gradient step (critic)\n",
    "        ddpg.logger.add_log(\"critic_loss\", critic_loss, ddpg.nb_steps)\n",
    "        ddpg.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ddpg.critic.parameters(), ddpg.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ddpg.critic_optimizer.step()\n",
    "\n",
    "        # Compute the actor loss\n",
    "        ddpg.t_actor(rb_workspace, t=0, n_steps=2)\n",
    "        ddpg.t_critic(rb_workspace, t=0, n_steps=2)\n",
    "        q_values = rb_workspace[\"critic/q_value\"]\n",
    "        actor_loss = compute_actor_loss(q_values)\n",
    "        \n",
    "\n",
    "        # Gradient step (actor)\n",
    "        ddpg.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ddpg.actor.parameters(), ddpg.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ddpg.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(\n",
    "            ddpg.critic, ddpg.target_critic, ddpg.cfg.algorithm.tau_target\n",
    "        )\n",
    "        \n",
    "        # Evaluate the actor if needed\n",
    "        if ddpg.evaluate():\n",
    "            if ddpg.cfg.plot_agents:\n",
    "                plot_policy(\n",
    "                    ddpg.actor,\n",
    "                    ddpg.eval_env,\n",
    "                    ddpg.best_reward,\n",
    "                    str(ddpg.base_dir / \"plots\"),\n",
    "                    ddpg.cfg.gym_env.env_name,\n",
    "                    stochastic=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2141f",
   "metadata": {},
   "source": [
    "## Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4c3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/ddpg-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 2,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.8,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 1500,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "            \n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-6,\n",
    "        # \"eps\": 5e-5,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-6,\n",
    "        # \"eps\": 5e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f665b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48a046f603040439d47c3cda7d1f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs/CartPoleContinuous-v1/ddpg-S2_20241014-160104/best_agent.mp4\n",
      "Moviepy - Building video /home/hatem/Cours/RL/mini-projects/project2/src/outputs/CartPoleContinuous-v1/ddpg-S2_20241014-160104/best_agent.mp4.\n",
      "Moviepy - Writing video /home/hatem/Cours/RL/mini-projects/project2/src/outputs/CartPoleContinuous-v1/ddpg-S2_20241014-160104/best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/hatem/Cours/RL/mini-projects/project2/src/outputs/CartPoleContinuous-v1/ddpg-S2_20241014-160104/best_agent.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><video src='data:video/mp4;base64,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' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg = DDPG(OmegaConf.create(params))\n",
    "run_ddpg(ddpg)\n",
    "ddpg.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594e0d4",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "You know have all the elements to study the impact of removing features from the environment\n",
    "on the training performance, and the impact of temporally extending the agent in mitigating \n",
    "partial observability, both with observation and with action extension.\n",
    "\n",
    "In practice, you should produce the following learning curves:\n",
    "\n",
    "- a learning curve of your algorithm on the standard CartPoleContinuous-v1 environment with full observability,\n",
    "- two learning curves, one from removing $\\dot{x}$ from CartPoleContinuous-v1 and the other from removing $\\dot{\\theta}$, \n",
    "- one learning curve from removing both $\\dot{x}$ and $\\dot{\\theta}$, \n",
    "- the same four learning curves as above, but adding each of the temporal extension wrappers, separately or combined.\n",
    "\n",
    "The way to combine these learning curves in different figures is open to you but should be carefully considered\n",
    "depending on the conclusions you want to draw. Beware of drawing conclusions from insufficient statistics.\n",
    "\n",
    "Discuss what you observe and conclude from this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794c75f",
   "metadata": {},
   "source": [
    "# Lab report\n",
    "\n",
    "Your report should contain:\n",
    "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
    "- in a separate pdf file with your names in the name of the file:\n",
    "    + a detailed enough description of all the choices you have made: the parameters you have set, the algorithms you have used, etc.,\n",
    "    + the curves obtained when doing Exercise 3,\n",
    "    + your conclusion from these experiments.\n",
    "\n",
    "Beyond the elements required in this report, any additional studies will be rewarded.\n",
    "For instance, you can extend the temporal horizon for the state memory and or action sequences beyond 2, and study the \n",
    "impact on learning performance and training time,\n",
    "you can play with other partially observable environments, etc.\n",
    "A great achievement would be to perform a comparison with the approach based on an LSTM."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
