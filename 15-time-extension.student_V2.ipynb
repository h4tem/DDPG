{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67841ce",
   "metadata": {},
   "source": [
    "- Permet de modifier les fichiers de la bib gymnasium pour debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acd5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48999453",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, you will study the effect of partial observability \n",
    "on the CartPoleContinuous-v1 environment, using either the DDPG or the TD3 algorithm.\n",
    "\n",
    "To emulate partial observability, you will design dedicated wrappers. Then you will study\n",
    "whether extending the input of the agent policy and critic with a memory of previous states\n",
    "and its output with action chunks can help solve the partial observability issue. This will\n",
    "also be achieved by designing other temporal extension wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39d97c",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbe005f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=False)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e0af3",
   "metadata": {},
   "source": [
    "# Temporal modification wrappers\n",
    "\n",
    "The CartPoleContinuous-v1 environment is a custom extension of \n",
    "[the CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/) \n",
    "with continuous actions, between -1 and 1. It is defined in the bbrl_gymnasium library.\n",
    "\n",
    "As in CartPole, the state of the system contains 4 variables:\n",
    "- the position $x$,\n",
    "- the velocity $\\dot{x}$,\n",
    "- the angle of the pole $\\theta$,\n",
    "- the angular velocity $\\dot{\\theta}$.\n",
    "\n",
    "To emulate partial observability in CartPoleContinuous-v1, you will hide the $\\dot{x}$ and $\\dot{\\theta}$ features, \n",
    "by filtering them out of the state of the environment. This is implemented with the ```FeatureFilterWrapper```.\n",
    "\n",
    "To compensate for partial observability, you will extend the architecture of the agent \n",
    "with a memory of previous states and its output with action chunks.\n",
    "This is implemented with to wrappers, the ```ObsTimeExtensionWrapper``` and the ```ActionTimeExtensionWrapper```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b49f3e",
   "metadata": {},
   "source": [
    "## The FeatureFilterWrapper\n",
    "\n",
    "The FeatureFilterWrapper removes a feature from the output observation when calling the ```reset()``` and ```step(action)``` functions.\n",
    "The index of the removed feature is given when building the object.\n",
    "\n",
    "To hide the $\\dot{x}$ and $\\dot{\\theta}$ features from the CartPoleContinuous-v1 environment, \n",
    "the idea is to call the wrapper twice, using something like\n",
    "```env = FeatureFilterWrapper(FeatureFilterWrapper(inner_env, 3), 1)``` where ```inner_env``` is the CartPoleContinuous-v1 environment.\n",
    "\n",
    "### Exercise 1: code the FeatureFilterWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the observation space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ba0c7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FeatureFilterWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, index):\n",
    "        super().__init__(env)\n",
    "        self.index = index\n",
    "        self.filter_observation()\n",
    "    \n",
    "    def filter_observation(self):\n",
    "        old_space = self.observation_space\n",
    "        low = np.delete(old_space.low, self.index)\n",
    "        high = np.delete(old_space.high, self.index)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=old_space.dtype)\n",
    "    \n",
    "    def observation(self,obs):\n",
    "        return np.delete(obs, self.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f63b4",
   "metadata": {},
   "source": [
    "## The ObsTimeExtensionWrapper\n",
    "\n",
    "When facing a partially observable environment, training with RL a reactive agent which just selects an action based on the current observation\n",
    "is not guaranteed to reach optimality. An option to mitigate this fundamental limitation is to equip the agent with a memory of the past.\n",
    "\n",
    "One way to do so is to use a recurrent neural network instead of a feedforward one to implement the agent: the neural network contains\n",
    "some memory capacity and the RL process may tune this internal memory so as to remember exactly what is necessary from the\n",
    "past observation. This has been done many times using an LSTM, see for instance \n",
    "[this early paper](https://proceedings.neurips.cc/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf).\n",
    "\n",
    "Another way to do so is to equip the agent with a list-like memory of the past observations \n",
    "and to extend the critic and policy to take as input the current observation and the previous ones.\n",
    "This removes the difficulty of learning an adequate representation of the past, but this results in \n",
    "enlarging the input size of the actor and critic networks. This can only be done if the required memory\n",
    "horizon to behave optimally is small enough.\n",
    "\n",
    "In the case of the CartPoleContinuous-v1 environment, one can see immediately that a memory of the previous\n",
    "observation is enough to compensate for the absence of the derivative features, since $\\dot{a} \\approx (a_{t} - a_{t-1})$.\n",
    "\n",
    "So we will extend the RL agent with a memory of size 1.\n",
    "\n",
    "Though it may not be intuitive at first glance, the simplest way to do so is to embed the environment into a wrapper\n",
    "which contains the required memory and produces the extended observations. This way, the RL agent will naturally be built\n",
    "with an extended observation space, and the wrapper will be in charge of concatenating the memorized\n",
    "observation from the previous step with the current observation received from the inner environment when calling the ```step(action)``` function.\n",
    "When calling the ```reset()``` function, the memory of observations should be reinitialized with null observations.\n",
    "\n",
    "### Exercise 2: code the ObsTimeExtensionWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the observation space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefbada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsTimeExtensionWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, history_length=2):\n",
    "        super().__init__(env)\n",
    "        self.history_length = history_length\n",
    "        self.history = []\n",
    "        low = np.repeat(env.observation_space.low, self.history_length)\n",
    "        high = np.repeat(env.observation_space.high, self.history_length)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=env.observation_space.dtype)\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        obs, info = self.env.reset(*args, **kwargs)\n",
    "        self.history = [np.zeros_like(obs)] * (self.history_length - 1)\n",
    "        self.history.append(obs)\n",
    "        return np.concatenate(self.history), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.history.append(obs)\n",
    "        self.history = self.history[-self.history_length:]\n",
    "        return np.concatenate(self.history), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b16d16",
   "metadata": {},
   "source": [
    "## The ActionTimeExtensionWrapper\n",
    "\n",
    "It has been observed that, in partially observable environments, preparing to play a sequence of actions and only playing\n",
    "the first can be better than only preparing for one action. The difference comes from the fact that the critic evaluates\n",
    "sequences of actions, even if only the first is played in practice.\n",
    "\n",
    "Similarly to the ObsTimeExtensionWrapper, the corresponding behavior can be implemented with a wrapper.\n",
    "The size of the action space of the extended environment should be \n",
    "M times the size of the action space of the inner environment. This ensures that the policy and the critic\n",
    "will consider extended actions.\n",
    "Besides, the ```step(action)``` function should receive an extended actions of size M times the size of an action,\n",
    "and should only transmit the first action to the inner environment.\n",
    "\n",
    "\n",
    "### Exercise 3: code the ActionTimeExtensionWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the action space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d0a90a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ActionTimeExtensionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, action_horizon=2):\n",
    "        super().__init__(env)\n",
    "        self.action_horizon = action_horizon\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.tile(env.action_space.low, self.action_horizon),\n",
    "            high=np.tile(env.action_space.high, self.action_horizon),\n",
    "            dtype=env.action_space.dtype\n",
    "        )\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c75f8",
   "metadata": {},
   "source": [
    "## Test the Wrappers with some Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7100da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    def __init__(self):\n",
    "        self.action = 0\n",
    "        self.env = gym.make(\"CartPoleContinuous-v1\")\n",
    "        self.obs_original, _ = self.env.reset(seed=0)\n",
    "        self.observation_original, _, _, _, _ = self.env.step(self.action)\n",
    "        \n",
    "\n",
    "    def test_featureFilterWrapper(self):\n",
    "        env_filtered = FeatureFilterWrapper(FeatureFilterWrapper(self.env, 3), 1)\n",
    "        obs_filtered, _ = env_filtered.reset(seed=0)\n",
    "        observation_filtered, _, _, _, _ = env_filtered.step(self.action)\n",
    "        self.obs_original = np.delete(self.obs_original, [1, 3])\n",
    "        self.observation_original = np.delete(self.observation_original, [1, 3])\n",
    "\n",
    "        assert np.all(self.obs_original == obs_filtered), f\"Expected {self.obs_original}, got {obs_filtered} in reset()\"\n",
    "        assert np.all(self.observation_original == observation_filtered), f\"Expected {self.observation_original}, got {observation_filtered} in action()\"\n",
    "\n",
    "    def test_obsTimeExtensionWrapper(self):\n",
    "        env_filtered = ObsTimeExtensionWrapper(FeatureFilterWrapper(FeatureFilterWrapper(self.env, 3), 1), 2)\n",
    "        obs_filtered, _ = env_filtered.reset(seed=0)\n",
    "        observation_filtered, _, _, _, _ = env_filtered.step(self.action)\n",
    "\n",
    "        assert np.all(self.obs_original == obs_filtered[2:]), f\"Expected {self.obs_original}, got {obs_filtered[2:]} in reset()\"\n",
    "        assert np.all(self.observation_original == observation_filtered[2:]), f\"Expected {self.observation_original}, got {observation_filtered[2:]} in action()\"\n",
    "\n",
    "        assert len(obs_filtered) == 4, f\"Expected 2, got {len(obs_filtered)}\"\n",
    "        assert len(observation_filtered) == 4, f\"Expected 2, got {len(observation_filtered)}\"\n",
    "\n",
    "    def test_actionTimeExtensionWrapper(self):\n",
    "        env_filtered = ActionTimeExtensionWrapper(ObsTimeExtensionWrapper(FeatureFilterWrapper(FeatureFilterWrapper(self.env, 3), 1)), action_horizon=3)\n",
    "        obs_filtered, _ = env_filtered.reset(seed=0)\n",
    "        self.action = [np.random.uniform(-1, 1), np.random.uniform(-1, 1), np.random.uniform(-1, 1)]\n",
    "        obs_filtered, _, _, _, _ = env_filtered.step(self.action)\n",
    "        assert env_filtered.action_space.shape[0] == 3, f\"Expected 3, got {len(env_filtered.action_space)}\"\n",
    "\n",
    "    def run(self):\n",
    "        print(\"[INFO] Running tests...\")\n",
    "        print(\"[INFO] Testing FeatureFilterWrapper...\")\n",
    "        self.test_featureFilterWrapper()\n",
    "        print(\"[INFO] Testing ObsTimeExtensionWrapper...\")\n",
    "        self.test_obsTimeExtensionWrapper()\n",
    "        print(\"[INFO] Testing ActionTimeExtensionWrapper...\")\n",
    "        self.test_actionTimeExtensionWrapper()\n",
    "        print(\"[INFO] All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467b4d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib backend: inline\n",
      "[INFO] Running tests...\n",
      "[INFO] Testing FeatureFilterWrapper...\n",
      "[INFO] Testing ObsTimeExtensionWrapper...\n",
      "[INFO] Testing ActionTimeExtensionWrapper...\n",
      "[INFO] All tests passed\n"
     ]
    }
   ],
   "source": [
    "test_class = Test()\n",
    "test_class.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17e8da",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ccd87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-36077ad610ff2761\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-36077ad610ff2761\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e5a03",
   "metadata": {},
   "source": [
    "# Experimental study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b21da",
   "metadata": {},
   "source": [
    "To run the experiments below, you can use the [DDPG](https://arxiv.org/pdf/1509.02971.pdf) \n",
    "or [TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf) algorithms.\n",
    "\n",
    "You can just copy paste here the code you have used during the corresponding labs.\n",
    "We only provide a suggested set of hyper-parameters working well on the CartPoleContinuous-v1 environment for DDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2141f",
   "metadata": {},
   "source": [
    "## Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4c3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/ddpg-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 2,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 8000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"action_noise\": 0.15,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "            # \"actor_hidden_size\": 256, # FOR LSTM\n",
    "            # \"critic_hidden_size\": 256, # FOR LSTM\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        # \"eps\": 5e-5,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        # \"eps\": 5e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594e0d4",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "You know have all the elements to study the impact of removing features from the environment\n",
    "on the training performance, and the impact of temporally extending the agent in mitigating \n",
    "partial observability, both with observation and with action extension.\n",
    "\n",
    "In practice, you should produce the following learning curves:\n",
    "\n",
    "- a learning curve of your algorithm on the standard CartPoleContinuous-v1 environment with full observability,\n",
    "- two learning curves, one from removing $\\dot{x}$ from CartPoleContinuous-v1 and the other from removing $\\dot{\\theta}$, \n",
    "- one learning curve from removing both $\\dot{x}$ and $\\dot{\\theta}$, \n",
    "- the same four learning curves as above, but adding each of the temporal extension wrappers, separately or combined.\n",
    "\n",
    "The way to combine these learning curves in different figures is open to you but should be carefully considered\n",
    "depending on the conclusions you want to draw. Beware of drawing conclusions from insufficient statistics.\n",
    "\n",
    "Discuss what you observe and conclude from this study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63c750",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4613a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ContinuousQAgent(Agent):\n",
    "#     def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "#         super().__init__()\n",
    "#         self.is_q_function = True\n",
    "#         self.lstm = nn.LSTM(state_dim + action_dim, hidden_layers[0], batch_first=True)\n",
    "#         self.fc = build_mlp([hidden_layers[0]] + list(hidden_layers[1:]) + [1], activation=nn.ReLU())\n",
    "\n",
    "#     def forward(self, t):\n",
    "#         obs = self.get((\"env/env_obs\", t))\n",
    "#         action = self.get((\"action\", t))\n",
    "#         # Pour avoir le format que le lstm prend\n",
    "#         obs_act = torch.cat((obs, action), dim=1).unsqueeze(0)  \n",
    "\n",
    "#         lstm_out, _ = self.lstm(obs_act)\n",
    "#         lstm_out = lstm_out.squeeze(0)  \n",
    "\n",
    "#         q_value = self.fc(lstm_out).squeeze(-1)\n",
    "#         self.set((f\"{self.prefix}q_value\", t), q_value)\n",
    "\n",
    "\n",
    "# class ContinuousDeterministicActor(Agent):\n",
    "#     def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(state_dim, hidden_layers[0], batch_first=True)\n",
    "#         self.fc = build_mlp([hidden_layers[0]] + list(hidden_layers[1:]) + [action_dim], activation=nn.ReLU(), output_activation=nn.Tanh())\n",
    "\n",
    "#     def forward(self, t, **kwargs):\n",
    "#         # Pour avoir le format que le LSTM prend\n",
    "#         obs = self.get((\"env/env_obs\", t)).unsqueeze(0) \n",
    "\n",
    "#         lstm_out, _ = self.lstm(obs)\n",
    "#         lstm_out = lstm_out.squeeze(0)  \n",
    "\n",
    "#         action = self.fc(lstm_out)\n",
    "#         self.set((\"action\", t), action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313222f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Get the current state $s_t$ and the chosen action $a_t$\n",
    "        obs = self.get((\"env/env_obs\", t))  # shape B x D_{obs}\n",
    "        action = self.get((\"action\", t))  # shape B x D_{action}\n",
    "\n",
    "        # Compute the Q-value(s_t, a_t)\n",
    "        obs_act = torch.cat((obs, action), dim=1)  # shape B x (D_{obs} + D_{action})\n",
    "        # Get the q-value (and remove the last dimension since it is a scalar)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)\n",
    "\n",
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4749cfa",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50482526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "\n",
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66547f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir /mnt/c/Cours-Sorbonne/M2/UE_DEEP/RLD/TME/PROJET_mini_2/outputs (started 0:00:09 ago; pid 2109)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488d63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "class Losses:\n",
    "    @staticmethod\n",
    "    def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values):\n",
    "        \"\"\"Compute the DDPG critic loss from a sample of transitions\n",
    "\n",
    "        :param cfg: The configuration\n",
    "        :param reward: The reward (shape 2xB)\n",
    "        :param must_bootstrap: Must bootstrap flag (shape 2xB)\n",
    "        :param q_values: The computed Q-values (shape 2xB)\n",
    "        :param target_q_values: The Q-values computed by the target critic (shape 2xB)\n",
    "        :return: the loss (a scalar)\n",
    "        \"\"\"\n",
    "        # Compute temporal difference\n",
    "        q_vals = q_values[0]\n",
    "        target_vals = target_q_values[1]\n",
    "\n",
    "        target = reward[1] + cfg[\"algorithm\"][\"discount_factor\"]*target_vals*must_bootstrap[1]\n",
    "        loss = mse(target, q_vals)\n",
    "        return loss\n",
    "        # assert False, 'Not implemented yet'\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_actor_loss(q_values):\n",
    "        \"\"\"Returns the actor loss\n",
    "\n",
    "        :param q_values: The q-values (shape 2xB)\n",
    "        :return: A scalar (the loss)\n",
    "        \"\"\"\n",
    "        # To be completed...\n",
    "        return torch.mean(q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d296f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UselessWrapper(gym.ObservationWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "    \n",
    "#     def reset(self, *args, **kwargs):\n",
    "#         return self.env.reset(*args, **kwargs)\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         return self.env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31055659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        env_wrappers = [\n",
    "            # retirer les velocity et angle de l'observation\n",
    "            # lambda env: FeatureFilterWrapper(env, 3),  \n",
    "            # lambda env: FeatureFilterWrapper(env, 1), \n",
    "            # Ajouter mémoire d'observation\n",
    "            # lambda env: ObsTimeExtensionWrapper(env, history_length=2),  \n",
    "            # Ajouter séquence d'actions\n",
    "            lambda env: ActionTimeExtensionWrapper(env, action_horizon=2),\n",
    "            # lambda env : UselessWrapper(env)  \n",
    "        ]\n",
    "        super().__init__(cfg, env_wrappers=env_wrappers)\n",
    "\n",
    "        # we create the critic and the actor, but also an exploration agent to\n",
    "        # add noise and a target critic. The version below does not use a target\n",
    "        # actor as it proved hard to tune, but such a target actor is used in\n",
    "        # the original paper.\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        print(\"Obs size : \", obs_size, \"Action size : \", act_size)\n",
    "        \n",
    "        self.critic = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic/\")\n",
    "        self.target_critic = copy.deepcopy(self.critic).with_prefix(\"target-critic/\")\n",
    "\n",
    "        self.actor = ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # As an alternative, you can use `AddOUNoise`\n",
    "        # noise_agent = AddOUNoise(cfg.algorithm.action_noise)\n",
    "        noise_agent = AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # Define agents over time\n",
    "        self.t_actor = TemporalAgent(self.actor)\n",
    "        self.t_critic = TemporalAgent(self.critic)\n",
    "        self.t_target_critic = TemporalAgent(self.target_critic)\n",
    "\n",
    "        # Configure the optimizer\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer = setup_optimizer(cfg.critic_optimizer, self.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f70cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2\n",
    "\n",
    "def run_ddpg(ddpg: DDPG):\n",
    "    for rb in ddpg.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(ddpg.cfg.algorithm.batch_size)\n",
    "\n",
    "        ddpg.t_actor(rb_workspace, t=0, n_steps=sequence_length)\n",
    "        ddpg.t_critic(rb_workspace, t=0, n_steps=sequence_length)\n",
    "        with torch.no_grad():\n",
    "            ddpg.t_target_critic(rb_workspace, t=0, n_steps=sequence_length)\n",
    "        \n",
    "\n",
    "        q_values, terminated, reward, action, target_q_values = rb_workspace[\n",
    "            \"critic/q_value\", \"env/terminated\", \"env/reward\", \"action\", \"target-critic/q_value\"\n",
    "        ]\n",
    "    \n",
    "        # Determines whether values of the critic should be propagated\n",
    "        must_bootstrap = ~terminated\n",
    "        critic_loss = Losses.compute_critic_loss(ddpg.cfg, reward, must_bootstrap, q_values, target_q_values)      \n",
    "\n",
    "        # Gradient step (critic)\n",
    "        ddpg.logger.add_log(\"critic_loss\", critic_loss, ddpg.nb_steps)\n",
    "        ddpg.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ddpg.critic.parameters(), ddpg.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ddpg.critic_optimizer.step()\n",
    "\n",
    "        # Compute the actor loss\n",
    "        \n",
    "        ddpg.t_actor(rb_workspace, t=0, n_steps=sequence_length)\n",
    "        ddpg.t_critic(rb_workspace, t=0, n_steps=sequence_length)\n",
    "        q_values = rb_workspace[\"critic/q_value\"]\n",
    "\n",
    "        actor_loss = Losses.compute_actor_loss(q_values)\n",
    "        # Gradient step (actor)\n",
    "        ddpg.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ddpg.actor.parameters(), ddpg.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ddpg.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(\n",
    "            ddpg.critic, ddpg.target_critic, ddpg.cfg.algorithm.tau_target\n",
    "        )\n",
    "        \n",
    "        # Evaluate the actor if needed\n",
    "        if ddpg.evaluate():\n",
    "            if ddpg.cfg.plot_agents:\n",
    "                plot_policy(\n",
    "                    ddpg.actor,\n",
    "                    ddpg.eval_env,\n",
    "                    ddpg.best_reward,\n",
    "                    str(ddpg.base_dir / \"plots\"),\n",
    "                    ddpg.cfg.gym_env.env_name,\n",
    "                    stochastic=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0270c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs size :  4 Action size :  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5310eb01cdd14b4a84ba0092a98ee875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddpg = DDPG(OmegaConf.create(params))\n",
    "run_ddpg(ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f78f7469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs/CartPoleContinuous-v1/ddpg-S2_20241015-170728/best_agent.mp4\n",
      "Moviepy - Building video /mnt/c/Cours-Sorbonne/M2/UE_DEEP/RLD/TME/PROJET_mini_2/outputs/CartPoleContinuous-v1/ddpg-S2_20241015-170728/best_agent.mp4.\n",
      "Moviepy - Writing video /mnt/c/Cours-Sorbonne/M2/UE_DEEP/RLD/TME/PROJET_mini_2/outputs/CartPoleContinuous-v1/ddpg-S2_20241015-170728/best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/bbrl_utils/algorithms.py:200\u001b[0m, in \u001b[0;36mRLBase.visualize_best\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_agent.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo of best agent recorded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m video_display(\u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mabsolute()))\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/bbrl/agents/gymnasium.py:83\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env, policy, path)\u001b[0m\n\u001b[1;32m     80\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     81\u001b[0m     t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:157\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m     clip \u001b[38;5;241m=\u001b[39m ImageSequenceClip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_sec)\n\u001b[1;32m    156\u001b[0m     moviepy_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_logger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_videofile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmoviepy_logger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# No frames captured. Set metadata.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/moviepy/decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    130\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[1;32m    131\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[1;32m    132\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    133\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip\u001b[38;5;241m.\u001b[39mismask:\n\u001b[1;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_audio:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m \u001b[43mffmpeg_write_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mwrite_logfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_logfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audiofile):\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_writer.py:213\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    211\u001b[0m     logfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m logger(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMoviepy - Writing video \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mFFMPEG_VideoWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    218\u001b[0m     nframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(clip\u001b[38;5;241m.\u001b[39mduration\u001b[38;5;241m*\u001b[39mfps)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t,frame \u001b[38;5;129;01min\u001b[39;00m clip\u001b[38;5;241m.\u001b[39miter_frames(logger\u001b[38;5;241m=\u001b[39mlogger, with_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m                                     fps\u001b[38;5;241m=\u001b[39mfps, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_writer.py:88\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.__init__\u001b[0;34m(self, filename, size, fps, codec, audiofile, preset, bitrate, withmask, logfile, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# order is important\u001b[39;00m\n\u001b[1;32m     80\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     get_setting(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFMPEG_BINARY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-y\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-loglevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logfile \u001b[38;5;241m==\u001b[39m sp\u001b[38;5;241m.\u001b[39mPIPE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-f\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-vcodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (size[\u001b[38;5;241m0\u001b[39m], size[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-pix_fmt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgba\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m withmask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%.02f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-an\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     90\u001b[0m ]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audiofile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, audiofile,\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-acodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     95\u001b[0m     ])\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "ddpg.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794c75f",
   "metadata": {},
   "source": [
    "# Lab report\n",
    "\n",
    "Your report should contain:\n",
    "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
    "- in a separate pdf file with your names in the name of the file:\n",
    "    + a detailed enough description of all the choices you have made: the parameters you have set, the algorithms you have used, etc.,\n",
    "    + the curves obtained when doing Exercise 3,\n",
    "    + your conclusion from these experiments.\n",
    "\n",
    "Beyond the elements required in this report, any additional studies will be rewarded.\n",
    "For instance, you can extend the temporal horizon for the state memory and or action sequences beyond 2, and study the \n",
    "impact on learning performance and training time,\n",
    "you can play with other partially observable environments, etc.\n",
    "A great achievement would be to perform a comparison with the approach based on an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd5696",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
